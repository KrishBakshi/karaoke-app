import pyaudio
import numpy as np
import time
import soundfile as sf
from scipy.signal import resample
import matplotlib.pyplot as plt
from collections import deque
import aubio

print("ğŸµ Priority Karaoke System (Smooth Instrumental)")

# Configuration - Optimized for smooth playback
CHUNK = 1024  # Larger buffer for smoother playback
RATE = 48000
FORMAT = pyaudio.paFloat32
CHANNELS = 1
INPUT_DEVICE = 6   # EarPods: USB Audio
OUTPUT_DEVICE = 6  # EarPods: USB Audio

# Preload WAV instrumental into memory
print("ğŸ¼ Preloading WAV instrumental...")
try:
    instr_audio, instr_sr = sf.read("blinding_lights_instrumental.wav", dtype='float32')
    print(f"âœ… Loaded {len(instr_audio)} samples at {instr_sr}Hz")
    
    # Ensure it's mono
    if len(instr_audio.shape) > 1:
        instr_audio = instr_audio[:, 0]
    
    # Create a circular buffer for smooth looping
    instr_pos = 0
    print("âœ… Audio preloaded into memory")
except Exception as e:
    print(f"âŒ Error loading WAV: {e}")
    exit(1)

def get_instrumental_chunk():
    global instr_pos
    end_pos = instr_pos + CHUNK
    
    if end_pos > len(instr_audio):
        # Need to wrap around
        chunk1 = instr_audio[instr_pos:]
        chunk2 = instr_audio[:end_pos - len(instr_audio)]
        chunk = np.concatenate([chunk1, chunk2])
        instr_pos = end_pos - len(instr_audio)
    else:
        chunk = instr_audio[instr_pos:end_pos]
        instr_pos = end_pos
    
    return chunk

# Setup audio
print("ğŸ”Š Setting up audio...")
p = pyaudio.PyAudio()

print(f"ğŸ¤ Using input device {INPUT_DEVICE} (EarPods)")
print(f"ğŸ”Š Using output device {OUTPUT_DEVICE} (EarPods)")
print(f"âš¡ Buffer size: {CHUNK} samples ({CHUNK/RATE*1000:.1f}ms latency)")

try:
    stream_in = p.open(format=FORMAT, channels=CHANNELS, rate=RATE,
                       input=True, input_device_index=INPUT_DEVICE,
                       frames_per_buffer=CHUNK)
    print("âœ… Input stream created")
except Exception as e:
    print(f"âŒ Input error: {e}")
    exit(1)

try:
    stream_out = p.open(format=FORMAT, channels=CHANNELS, rate=RATE,
                        output=True, output_device_index=OUTPUT_DEVICE,
                        frames_per_buffer=CHUNK)
    print("âœ… Output stream created")
except Exception as e:
    print(f"âŒ Output error: {e}")
    exit(1)

# Pitch detector (less frequent processing)
pitch_o = aubio.pitch("yin", CHUNK * 2, CHUNK, RATE)
pitch_o.set_unit("Hz")
pitch_o.set_silence(-40)

# Simple plot (optional - can be disabled)
print("ğŸ“Š Setting up plot...")
fig, ax = plt.subplots()
pitch_history = deque(maxlen=50)
time_history = deque(maxlen=50)
line, = ax.plot([], [])
ax.set_ylim(50, 800)
ax.set_xlabel("Time (s)")
ax.set_ylabel("Pitch (Hz)")
ax.set_title("Your Pitch (Priority)")
plt.tight_layout()
plt.show(block=False)

print("ğŸ¤ Starting priority karaoke! Sing into your microphone...")
print("ğŸ’¡ Instrumental should play smoothly with minimal lag")
print("ğŸ“Š You should see a plot of your pitch")
print("ğŸ›‘ Press Ctrl+C to stop")

start_time = time.time()
frame_count = 0

try:
    while True:
        elapsed = time.time() - start_time
        frame_count += 1
        
        # Read microphone
        data = stream_in.read(CHUNK, exception_on_overflow=False)
        samples = np.frombuffer(data, dtype=np.float32)
        
        # Get instrumental chunk FIRST (priority)
        instrumental = get_instrumental_chunk()
        
        # Detect pitch (much less frequently to reduce CPU load)
        if frame_count % 16 == 0:  # Only detect pitch every 16th frame
            pitch = pitch_o(samples)[0]
            confidence = pitch_o.get_confidence()
        else:
            pitch = 0
            confidence = 0
        
        # Simple autotune (only when confidence is high)
        if confidence > 0.8 and pitch > 0:
            # Find nearest note frequency
            note_freqs = [261.63, 293.66, 329.63, 349.23, 392.00, 440.00, 493.88, 523.25]  # C4 to C5
            nearest_note = min(note_freqs, key=lambda x: abs(x - pitch))
            shift_ratio = nearest_note / pitch
            tuned_samples = resample(samples, int(len(samples) * shift_ratio))
            if len(tuned_samples) < CHUNK:
                tuned_samples = np.pad(tuned_samples, (0, CHUNK - len(tuned_samples)))
            else:
                tuned_samples = tuned_samples[:CHUNK]
        else:
            tuned_samples = samples
        
        # Mix with instrumental (prioritize instrumental)
        mix = 0.8 * instrumental + 0.2 * tuned_samples  # Much more instrumental
        mix = np.clip(mix, -1.0, 1.0)
        
        # Output
        stream_out.write(mix.astype(np.float32).tobytes())
        
        # Update plot (much less frequently)
        if frame_count % 80 == 0:
            time_history.append(elapsed)
            pitch_history.append(pitch)
            
            if len(time_history) > 1:
                line.set_data(list(time_history), list(pitch_history))
                ax.set_xlim(max(0, elapsed - 5), elapsed + 0.5)
                plt.pause(0.001)
        
        # Debug output (much less frequent)
        if frame_count % 800 == 0:
            print(f"â±ï¸  {elapsed:.1f}s | ğŸ¤ Pitch: {pitch:.1f}Hz | Confidence: {confidence:.2f}")

except KeyboardInterrupt:
    print("\nğŸ›‘ Stopping karaoke...")

finally:
    stream_in.stop_stream()
    stream_out.stop_stream()
    stream_in.close()
    stream_out.close()
    p.terminate()
    print("âœ… Cleanup complete!") 